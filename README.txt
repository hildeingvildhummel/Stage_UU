***Estimating Pain in Horses and Donkeys using Facial Expressions***
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Directories:
- all_images_together: Contains all the images of the dataset as jpg file
- all_landmarks_together: Contains all the landmarks per image as txt file
- darknet: Contains all the requirement files for YOLOV3 detection and training
- Final: Contains all the dependencies and outputs of the project 
	- CNN_features: Contains all the extracted CNN features per image saved as numpy
	- dataset: 
	- Evaluation: Contains all the generated results
		- total_score: results of the total pain score prediction
		- fusion: results of the fusion of the results  
	- figures: Contains all the generated figures 
		- analysis: figures of the agreement between experts 
		- data: figures of the dataset 
		- noise: figures of the sensitivity analysis 
		- total: figures of the score
		- YOLO: figures of the YOLOV3 face detection
	- HOG_features: Contains all the extracted HOG features per image saved as numpy
	- indices_horse: Contains txt files with the augmentated photonumbers of the train set 
	- labels_horse: Contains the mean shapes as txt files and the labels assigned to every photonumber after Kmeans clustering as txt file
	- LBP_features: Contains the extracted LBP features per image saved as numpy 
	- models: Contains all the trained models as sav files
		- single_model: contains the trained models using a single mean shape
	- noise_performance: Contains excel files with the results generated with the noisy test set 
	- noisy_landmarks: Contains the augmentated landmark files, as numpy 
		- The folders within this directory are the noise induced test set landmark files 
	- raw_results: Contains excel files generated by the models with every prediction 
		- single_model: containes the predictions of the single model
	- SIFT_features: Contains all the extracted SIFT fetaures per image saved as numpy 
	- split: Contains the train and test set splitted as csv 

- pykernels: Contains the kernels used for the SVM
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
The main_pipeline.py is the overall script calling the functions needed to train 
and test the models. 
The pipeline.py runs the complete pipeline. 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Excel files:
- ALL_POSITIONS: Containing the head pose and missing landmarks per image 
- intra_Thijs_horse_thijs: The pain scores assigned for the second time by a veterinarian expert 
- paard_amber: Horse pain scores given by the second veterinarian expert 
- paard_Thijs: Horse pain scores given by the first veterinarian expert 
- paard_whitney: Horse pain scores given by the third veterinarian expert 
- Thijs_horse_and_donkey: All the pain scores given by the first veterinarian expert 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
darknet_no_gpu is the application needed to run the darknet models for Windows 
The obj.data, obj.names, pthreadVC2.dll, yolov3-horse.cfg and yolov3-horse_4000.weights are the files that the
darknet model depend on. The train.txt is used to train the model and the test.txt is used to test the model. 
The darknet training and testing approach used for this project is described in:
https://github.com/AlexeyAB/darknet
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
The scripts, create_pts_files.py, get_data.py and pose_classifier are written by the person working on the parallel project
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


