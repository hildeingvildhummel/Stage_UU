import pandas as pd
import numpy as np
from functools import reduce
import random
from sklearn.svm import SVR
import yaml
import math
import itertools
from pykernels.pykernels.regular import Min, GeneralizedHistogramIntersection
from pykernels.pykernels.basic import Linear
from sklearn.model_selection import GroupShuffleSplit, ParameterGrid
from sklearn.linear_model import LinearRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error, f1_score
import statistics
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold, ParameterGrid, GridSearchCV
from sklearn.neighbors import KernelDensity
import pickle
import os
from sklearn.metrics import make_scorer, confusion_matrix
from statistics import mean
from train_test_val_split import training_val_split
from donkey_classifier import get_rounded, f1_scorer


def mse_classifictaion_fusion(true_values, prediction):
    """This function calculates the Mean squared error of the binarized results generated by fusion.
    Input
    - true_values: the true pain scores
    - prediction: The predicted pain scores by fusion
    Output:
    - score: The mse of the fused results
    """
    #initiate empty lists
    true_list = []
    pred_list = []
    #round the values to the nearest pain score and binarize the results
    true_values = get_rounded(true_values)
    prediction = get_rounded(prediction)
    for i in true_values:
        if int(i) == 0:
            true_list.append([0,0])
        elif int(i) == 1:
            true_list.append([1,0])
        elif int(i) == 2:
            true_list.append([1,1])
        else:
            print('except true')
    #same for the predicted results
    for i in prediction:
        if int(i) == 0:
            pred_list.append([0,0])
        elif int(i) == 1:
            pred_list.append([1,0])
        elif int(i) == 2:
            pred_list.append([1,1])
        else:
            print('except predict')
    #calculate the mean squared error
    score = mean_squared_error(np.array(true_list), np.array(pred_list))
    #return the MSE
    return score

def fuse_results():
    """This function fuses the results of the models trained with either HOG, SIFT or CNN features by a simple fusion and a simple weighted fusion. It saves the result in a given txt file """

    #List of pain features
    expression_list =  ['Ears', 'Orbital tightning', 'Angulated upper eyelid', 'Sclera', 'Corners of the mouth', 'Nostrils']
    #Excel file containing the 'true' pain scores
    scores = pd.read_excel('Thijs_horse_and_donkey.xlsx')
    #List of the head poses
    head_pose = ['side', 'front', 'tilted']
    #Iterate over all the given head poses and select the features within the head pose
    for str_head_pose in head_pose:
        if str_head_pose == 'front':
            features = [2, 3, 4, 5, 7]
            #otherwise, predict all pain features
        else:
            features = [2, 3, 4, 5, 6, 7]
        #Iterate over all the pain features
        for predict_feature_index in features:
            #Open all the files containing the predictions per model trained on a given feature extraction method
            results_hog = pd.read_csv('Final/raw_results/train_scores_%s_%s_HOG_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))

            results_sift = pd.read_csv('Final/raw_results/train_scores_%s_%s_SIFT_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))

            results_cnn = pd.read_csv('Final/raw_results/train_scores_%s_%s_CNN_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))

            #concatenate the predictions into a single dataframe
            sub_1 = pd.concat([results_hog, results_sift], axis = 1, sort = False)
            results = pd.concat([sub_1, results_cnn], axis=1, sort=False)
            #Remove the duplicate columns
            results = results.loc[:,~results.columns.duplicated()]
            #Open the pain scores of the training scores
            y_training = pd.read_csv('Final/split/%s_train_horse.csv' %(str_head_pose))
            #Repeat the rows 3 times, due to augmentation
            y_training_id = pd.DataFrame(np.repeat(y_training.values, 3, axis = 0))
            y_training_id.columns = y_training.columns
            #Select the column with the right pain scores given the pain feature
            y_training = y_training.iloc[:, predict_feature_index]


            #Repeat these scores 3 times as well, due to augmentation
            y_training = np.repeat(y_training, 3)
            #Reset the index of the y_train
            y_train = y_training.reset_index(drop = True)
            #open the generated test scores by the models trained with the different features extracted as well
            results_hog = pd.read_csv('Final/raw_results/test_scores_%s_%s_HOG_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))
            results_sift = pd.read_csv('Final/raw_results/test_scores_%s_%s_SIFT_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))
            results_cnn = pd.read_csv('Final/raw_results/test_scores_%s_%s_CNN_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))
            #Concatenate the test predictions
            sub_1 = pd.concat([results_hog, results_sift], axis = 1, sort = False)
            test_results = pd.concat([sub_1, results_cnn], axis = 1, sort =False)
            #Remove duplicate columns
            test_results = test_results.loc[:,~test_results.columns.duplicated()]
            #Open the test scores
            y_test =pd.read_csv('Final/split/%s_test_horse.csv' %(str_head_pose))
            #Select the corresponding test pain scores
            y_test = y_test.iloc[:, predict_feature_index]
            #reset the index of y_test
            y_test = y_test.reset_index(drop=True)
            #Select the indices with missing values in the test set scores and save it to a list
            missing_test = y_test[y_test.isnull()].index.tolist()
            #Drop the missing values
            y_test = y_test.drop(y_test.index[[missing_test]], axis = 0)
            #Do the same for the training set scores
            missing = y_train[y_train.isnull()].index.tolist()
            y_train = y_train.drop(y_train.index[[missing]], axis = 0)
            #Drop the first 2 columns of train and test set
            results = results.drop(results.columns[[0, 1]], axis = 1)
            test_results = test_results.drop(test_results.columns[[0, 1]], axis = 1)
            #Drop the missing values from the training scores as well and reset the index
            y_training_id = y_training_id.drop(y_training_id.index[[missing]], axis = 0)
            y_training_id = y_training_id.reset_index(drop=True)
            print(len(y_training_id), len(y_train))



            #try to open a pre-trained model
            try:
                fusion_model = pickle.load(open('Final/models/fusion_%s_%s.sav' % (str_head_pose, expression_list[predict_feature_index - 2], 'rb')))
            #if not possible, train a new model
            except:
                #Create all possible parameter combinations
                params = ParameterGrid({'normalize': [True, False]})
                #Initiate an empty list
                indices = []
                #select a high value as the best error
                best_score = 10000000000000000000
                #Set iteration to 0
                k = 0
                #Iterate 5 times
                while k < 5:
                    print('iteration: ', k)
                    print(y_training_id.head(5))
                    #initiate empty lists to save train-validation split
                    indices_X_train = []
                    indices_X_val = []
                    #Make the split
                    y_val, y_train_val, indices_val, indices_train_val = training_val_split(y_training_id, indices)
                    #Append the training and validation indices to the lists
                    indices_X_val.append(y_training_id.index[y_training_id['photonumber'].isin(np.unique(indices_val))])
                    indices_X_train.append(y_training_id.index[y_training_id['photonumber'].isin(np.unique(indices_train_val))])
                    #Select the values of both the training and the validation set
                    X_train_val = results[results.index.isin(np.concatenate(indices_X_train, axis = 0))]
                    X_val = results[results.index.isin(np.concatenate(indices_X_val, axis = 0))]
                    #Iterate over the parameters
                    for param in params:
                        #Initiate a LinearRegression model with the given parameters
                        linear = LinearRegression(**param)
                        #Fit the model
                        linear.fit(X_train_val, y_train_val.iloc[:, predict_feature_index])
                        #Predict the validation set
                        prediction_val = linear.predict(X_val)
                        #Calculate the MSE of the binarized scores
                        val_error = mse_classifictaion_fusion(y_val.iloc[:, predict_feature_index], prediction_val)
                        #If the error is lower than the best score..
                        if val_error < best_score:
                            #Save the parameters and save the error as the new best score
                            best_params = param
                            best_score = val_error
                    print(best_score)
                    #The validation indices are set as the new indices list
                    indices = indices_val
                    #Add 1 to the iteration number
                    k += 1
                #Train the final model with the selected parameters
                fusion_model = LinearRegression(**best_params)
                #Fit it to the whole training set
                fusion_model.fit(results, y_train)
                #Save the model
                filename = 'Final/models/fusion_%s_%s.sav' % (str_head_pose, expression_list[predict_feature_index - 2])
                pickle.dump(fusion_model, open(filename, 'wb'))
            #predict the test set
            weighted_result = fusion_model.predict(test_results)
            #Calculate the MSE of the binarized results
            weighted_f1 = mse_classifictaion_fusion(y_test, weighted_result)
            print(y_test)
            print(results.mean())
            #Calculate the MSE of the binarized results using a simple fusion: min and mean
            mean_f1 = mse_classifictaion_fusion(y_test, test_results.mean(axis = 1))
            # max_f1 = mse_classifictaion_fusion(y_test, test_results.max(axis = 1))
            min_f1 = mse_classifictaion_fusion(y_test, test_results.min(axis = 1))

            #Save the weighted model
            if os.path.isdir('Final/Evaluation/fusion') == False:
                os.makedirs('Final/Evaluation/fusion')
            #create a file
            file = open('Final/Evaluation/fusion/final_fusion_simple_weighted_%s_%s_horse_2.txt' % (str_head_pose, expression_list[predict_feature_index - 2]), 'w')
            #round the prediction to the nearest pain score
            rounded_test = get_rounded(weighted_result)
            #create a confusion matrix
            confusion = confusion_matrix(y_test,rounded_test)
            #convert confusion matrix to string
            cm = np.array2string(confusion)
            #Save the results
            file.write('Confusion Matrix\n\n{}\n'.format(cm))
            file.write('Weighted \n{}\n'.format(weighted_f1))
            file.write('Min \n{}\n'.format(min_f1))
            # file.write('Max \n{}\n'.format(max_f1))
            file.write('Mean \n{}\n'.format(mean_f1))

            file.close()

    return weighted_result


def find_wrong_horses():
    """This function filters the wrongly classified horses and compares these with the other 2 experts
    Output:
    per head pose a plot is shown showing the agreement between the experts about the wrongly classified horses"""
    #List of head poses
    head_poses = ['tilted', 'front', 'side']
    #Initiate empty lists
    ears = []
    orbital = []
    eyelid = []
    sclera = []
    nostrils = []
    mouth = []
    #List of pain features
    expression_list = ['Ears', 'Orbital tightning', 'Angulated upper eyelid', 'Sclera', 'Corners of the mouth', 'Nostrils']
    #Iterate over all the head poses
    for str_head_pose in head_poses:
        #Open the true pain scores
        true_values = pd.read_csv('Final/split/%s_test_horse.csv' % (str_head_pose))
        #Open the pain scores of the other experts
        whitney_values = pd.read_excel('paard_whitney.xlsx')
        amber_values = pd.read_excel('paard_amber.xlsx')
        #Initate an empty dataframe
        experts_opinion = pd.DataFrame()
        #Iterate over the pain features
        for predict_feature_index in list(range(2, 8)):
            #Skip the mouth feature if the front head pose is given
            if str_head_pose == 'front' and predict_feature_index == 6:
                continue
            print(str_head_pose, expression_list[predict_feature_index - 2])
            #Fuse the results of all the extracted feature types given the weights
            FUSION = pickle.load(open('Final/models/fusion_%s_%s.sav' % (str_head_pose, expression_list[predict_feature_index - 2]), 'rb'))
            predicted_HOG = pd.read_csv('Final/raw_results/test_scores_%s_%s_HOG_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))
            predicted_SIFT = pd.read_csv('Final/raw_results/test_scores_%s_%s_SIFT_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))
            predicted_CNN = pd.read_csv('Final/raw_results/test_scores_%s_%s_CNN_horse.csv' % (str_head_pose, expression_list[predict_feature_index - 2]))
            #Concatenate the predictions
            sub_1 = pd.concat([predicted_HOG, predicted_SIFT], axis = 1, sort = False)
            results = pd.concat([sub_1, predicted_CNN], axis=1, sort=False)
            #Remove duplicate columns
            results = results.loc[:,~results.columns.duplicated()]
            #Drop the final 3 columns
            results =  results[results.columns[-3:]]
            #Get the fused results
            predicted = FUSION.predict(results)
            #select the missing scores
            index_test = true_values.iloc[:, predict_feature_index].index[true_values.iloc[:, predict_feature_index].apply(np.isnan)]
            predicted_true = true_values.iloc[:, predict_feature_index]
            predicted_true = predicted_true.dropna()

            # Calculate the difference between the true values and the predicted values
            error_array = predicted_true.array - predicted.astype(int)
            #Save the inidices for the wrongly classified horses
            indices = np.nonzero(error_array)
            #Inititate empty lists
            wrong_horses = []
            whitney_prediction = []
            amber_prediction = []
            true_prediction = []
            #Iterate over the wrongly classified horses
            for i in indices[0]:
                #save the photonumbers
                wrong_horses.append(true_values.iloc[i, 0])
                #save scores of the second expert
                whitney_prediction.append(whitney_values.iloc[i, predict_feature_index])
                #Save scores of the third expert
                amber_prediction.append(amber_values.iloc[i, predict_feature_index])
                #Save the 'true' values of the first expert
                true_prediction.append(true_values.iloc[i, predict_feature_index])
            #get the difference between the first expert and the second/thrid expert
            whitney_error_array = np.array(whitney_prediction) - np.array(true_prediction)
            amber_error_array = np.array(amber_prediction) - np.array(true_prediction)
            #Save the indices of non-zero values
            indices_whitney = np.nonzero(whitney_error_array)
            indices_amber = np.nonzero(amber_error_array)
            #Initiate an empty list
            new_array = []
            #Iterate over the disagreement with both experts and append the index to the list
            for i in indices_whitney[0]:
                new_array.append(i)
            for i in indices_amber[0]:
                new_array.append(i)
            #Only save the unique indices
            new_array = np.unique(np.array(new_array))

            #Assign a variable to the number of agreements and disagreements per pain feature
            if predict_feature_index == 2:
                ears_agreed = len(indices[0]) - len(new_array)
                ears_not_agreed = len(new_array)
                print(ears_agreed, ears_not_agreed)
                ears = wrong_horses
            elif predict_feature_index == 3:
                orbital_agreed = len(indices[0]) - len(new_array)
                orbital_not_agreed = len(new_array)
                orbital = wrong_horses
            elif predict_feature_index == 4:
                eyelid_agreed = len(indices[0]) - len(new_array)
                eyelid_not_agreed = len(new_array)
                eyelid = wrong_horses
            elif predict_feature_index == 5:
                sclera_agreed = len(indices[0]) - len(new_array)
                sclera_not_agreed = len(indices)
                sclera = wrong_horses
            elif predict_feature_index == 6:
                mouth_agreed = len(indices[0]) - len(new_array)
                mouth_not_agreed = len(indices)
                mouth = wrong_horses
            elif predict_feature_index == 7:
                nose_agreed = len(indices[0]) - len(new_array)
                nose_not_agreed = len(indices)
                nose = wrong_horses
        #Set names for the plot based on the head pose
        if str_head_pose == 'front':
            n_groups = 5
            agreed = [ears_agreed, orbital_agreed, eyelid_agreed, sclera_agreed, nose_agreed]
            not_agreed = [ears_not_agreed, orbital_not_agreed, eyelid_not_agreed, sclera_not_agreed, nose_not_agreed]
        else:
            n_groups = 6
            agreed = [ears_agreed, orbital_agreed, eyelid_agreed, sclera_agreed, mouth_agreed, nose_agreed]
            not_agreed = [ears_not_agreed, orbital_not_agreed, eyelid_not_agreed, sclera_not_agreed, mouth_not_agreed, nose_not_agreed]

        # create plot
        fig, ax = plt.subplots()
        index = np.arange(n_groups)
        bar_width = 0.35
        opacity = 0.8

        rects1 = plt.bar(index, agreed, bar_width,
        alpha=opacity,
        color='b',
        label='Agreed')

        rects2 = plt.bar(index + bar_width, not_agreed, bar_width,
        alpha=opacity,
        color='g',
        label='Not-Agreed')

        plt.xlabel('Pain Feature')
        plt.ylabel('Frequency')

        if str_head_pose == 'front':
            plt.xticks(index + bar_width, ('Ears', 'Orbital', 'Eyelid', 'Sclera', 'Mouth', 'Nostrils'))
        else:
            plt.xticks(index + bar_width, ('Ears', 'Orbital', 'Eyelid', 'Sclera', 'Mouth', 'Nostrils'))
        plt.legend()

        plt.tight_layout()
        plt.show()
        plt.plot()
        #Check if there are wrongly classified horses with non of the pain features correctly classified
        total_mis = set(ears) & set(orbital) & set(eyelid) & set(sclera) & set(mouth) & set(nose)
        print(total_mis)
        for i in total_mis:
            print(true_values.iloc[i, 0])
